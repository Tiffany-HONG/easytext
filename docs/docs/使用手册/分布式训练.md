# 分布式训练

主要实践在: 单机多卡。

分布式训练的思想是: 基于多进程，将数据拆分到不同的进程，进行训练。Pytorch 提供了在不同的进程中
对参数进行合并的操作。

基于上面的思想，在写分布式训练的时候，需要解决的一些组件包括:

1. 分布式下的 `DataLoader` 如何运行?
2. 分布式下的 `Model` 如何参数更新?
3. 分布式下的 `Loss` 如何计算?
4. 分布式下的 `Metric` 如何计算?

## 分布式下的 `DataLoader` 如何运行?

使用 `torch.utils.data.distributed.DistributedSampler` 根据当前进程对 dataset 进行采样。
类似下面这段代码:

```python

import torch

def create_dataloader(distributed: bool, dataset: torch.data.Dataset, batch_size: int):
    if distributed:
        sampler = torch.utils.data.distributed.DistributedSampler(dataset)
    else:
        sampler = None
    
    data_loader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=(sampler is None), sampler=sampler)
    return data_loader
```


## 分布式下的 `Model` 如何参数更新?
Model 使用 `torch.nn.parallel.DistributedDataParallel(model)` 设置，会自动多进程进行参数的更新。

## 分布式下的 `Loss` 如何计算?
Loss 会在 Trainer 中自动进行 分布式计算，所以正常写即可。
## 分布式下的 `Metric` 如何计算?

Metric 需要自己定义分布式该如何处理。